#!/usr/bin/env python
# Author : Scott Adams
# My Python re-write of Kochanek's dustymc2
# Requires the following in a directory to run properly:
#    dusty
#    dusty.inp
#    lambda_grid.dat
# testing it out here: tau1_mycode/

# Edits:
#    3 Nov 2014 - reduced MCMC step sizes for Tdust and Tstar
#   11 Nov 2014 - added options to extrapolate a shell dust model forwards/backwards in time
#   14 Nov 2014 - Added option to include limit on L from dL/dt in chi^2
#    7 Jan 2015 - Added option to change grain size distribution limits a(min) and a(max)
#   14 Jan 2015 - Changed dL/dt chi^2 to utilize the effective (rather than total) optical depth
#    6 Mar 2015 - Added foreground extinction parameter to MCMC
#   11 Apr 2015 - Added capability to employ variability constraints from multiple filters
#   13 Apr 2015 - Adding alternative variability constraint utilizing a light curve rather than dL/dt
#   29 Apr 2015 - Edited extrapolation mode to MCMC through dust temperature
#	(rather than just assuming the temperature of the model that is being
#	extrapolated from)
#   13 May 2015 - Edited variability constraints to allow for ref image made from pre-eruption data
#   28 May 2015 - Added knob for adjusting MCMC step sizes up or down
#   22 Sep 2015 - Added option to fix Lstar
#   14 Mar 2016 - Fixed bug in variability constraints array masking
#   10 Jul 2017 - Added option to use blackbody instead of stellar template
#   20 Jul 2017 - changed to make velocity constraint apply to outer (instead of inner) dust radius
#   27 Sep 2017 - Fixed bug in calculate_tau_coeff()
#    1 Mar 2018 - Added option to print model mags/fluxes in a set of filters

import numpy as np
from numpy.random import normal as gasdev
from numpy.random import uniform as rand
from os import system
from sys import exit, argv
from time import time
from datetime import datetime
import argparse
# Non-standard modules:
import jdcal
from astropy.io import ascii
# Personal library modules:
#from mylib import return_ascii_starting_header, read_header

def read_header(header,field):
  """for HEADER with content:
  FIELD1 = VALUE1 % DESCRIPTION1
  FIELD2 = VALUE2 % DESCRIPTION2
  read_header(HEADER, FIELD1) -> VALUE1"""
  from sys import exit
  for i in xrange(len(header)):
    line = header[i].strip('# ')
    sline = line.split(' = ')
    if sline[0] == field:
      trimmed_line = sline[1].strip('\n')
      return trimmed_line.split('%')[0]
  print "ERROR: '%s' not found in header" % (field)
  exit(1)

def return_ascii_starting_header(filename, comment_marker):
  header = []
  for line in file(filename):
    if line.startswith('#'):
      header.append(line)
    else:
      return header
  return header

def rl(rv,x):
  "rv is typically 3.1, x is 1/lambda [microns^-1]"
  if x < 1.1:
     a =  0.574*x**1.61
     b = -0.527*x**1.61
  else:
    if x < 3.3:
      y = x-1.82
      a = (1.0+0.17699*y-0.50447*y*y-0.02427*y**3+0.72085*y**4 +
          0.01979*y**5-0.77530*y**6+0.32999*y**7)
      b = (1.41338*y+2.28305*y**2+1.07233*y**3-5.38434*y**4-
          0.62251*y**5+5.30260*y**6-2.09002*y**7)
    else:
      if x < 5.9:
        fa = 0.0
        fb = 0.0
      else:
        fa = -0.04473*(x-5.9)**2 - 0.009779*(x-5.9)**3
        fb =  0.21300*(x-5.9)**2 + 0.120700*(x-5.9)**3
      a =  1.752 - 0.316*x - 0.104/((x-4.67)**2+0.341) + fa
      b = -3.090 + 1.825*x + 1.206/((x-4.67)**2+0.263) + fb
  rlval = rv*(a+b/rv)
  return rlval

def calculate_tau_coeff():
  coefficient_data = ascii.read('foo1.stb')
  # find the wavelength closest to that of dLdt_filter
  lambda_diff = np.abs(coefficient_data['col1'] - args.effective_optical_depth_lambda)
  index = np.argmin(lambda_diff)
  if lambda_diff[index] > 0.01:
    print 'ERROR: effective_optical_depth_lambda not contained within foo1.stb'
    exit(1)
  albedo = coefficient_data['col8'][index]
  tau_coeff = (1.0 - albedo)**0.5
  if verbose:
    print 'albedo = %s' % (albedo)
    print 'tau_eff coefficient = %s' % (tau_coeff)
  # calculate tau_effective ratios
  tau_ratios = []
  tau_optical = coefficient_data['col7'][index]*tau_coeff
  for wavelength in dLdt_lambdas:
    tmp_lambda_diff = np.abs(coefficient_data['col1'] - wavelength)
    tmp_index = np.argmin(tmp_lambda_diff)
    if tmp_lambda_diff[tmp_index] > 0.1:
      print 'ERROR: %0.2f microns lies outside of foo1.stb spectrum' % (wavelength)
      exit(1)
    tmp_albedo = coefficient_data['col8'][tmp_index]
    tmp_tau_coeff = (1.0 - tmp_albedo)**0.5
    tau_wavelength = coefficient_data['col7'][tmp_index]*tmp_tau_coeff
    tau_ratio = tau_wavelength / tau_optical
    tau_ratios.append(tau_ratio)
    if verbose:
      print 'tau_eff(%0.2f-micron)/tau_eff(%0.2f-micron) = %0.2f' % (wavelength,args.effective_optical_depth_lambda,tau_ratio)
  return tau_coeff, np.array(tau_ratios)

def readkurucz():
  flist = []
  # these are generated by SMOOTH2.PL in MARCS
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t3500g40k2odfnew.dat') 
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t3750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t4000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t4250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t4500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t4750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t5000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t5250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t5500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t5750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t6000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t6250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t6500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t6750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t7000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t7250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t7500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t7750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t8000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t8250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t8500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t8750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t9000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t9250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t9500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t9750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t10000g40k2odfnew.dat') 
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t10250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t10500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t10750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t11000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t11250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t11500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t11750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t12000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t12250g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t12500g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t12750g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t13000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t14000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t15000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t16000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t17000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t18000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t19000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t20000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t21000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t22000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t23000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t24000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t25000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t26000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t27000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t28000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t29000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t30000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t31000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t32000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t33000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t34000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t35000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t36000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t37000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t38000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t39000g40k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t40000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t41000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t42000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t43000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t44000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t45000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t46000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t47000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t48000g45k2odfnew.dat')
  flist.append(path_to_filter_files+'kuruczf'+args.metallicity+'t49000g45k2odfnew.dat')
  tlist = []
  tlist.append(3500) 
  tlist.append(3750)
  tlist.append(4000)
  tlist.append(4250)
  tlist.append(4500)
  tlist.append(4750)
  tlist.append(5000)
  tlist.append(5250)
  tlist.append(5500)
  tlist.append(5750)
  tlist.append(6000) 
  tlist.append(6250)
  tlist.append(6500)
  tlist.append(6750)
  tlist.append(7000)
  tlist.append(7250)
  tlist.append(7500)
  tlist.append(7750)
  tlist.append(8000)
  tlist.append(8250)
  tlist.append(8500)
  tlist.append(8750)
  tlist.append(9000)
  tlist.append(9250)
  tlist.append(9500)
  tlist.append(9750)
  tlist.append(10000) 
  tlist.append(10250)
  tlist.append(10500)
  tlist.append(10750)
  tlist.append(11000)
  tlist.append(11250)
  tlist.append(11500)
  tlist.append(11750)
  tlist.append(12000)
  tlist.append(12250)
  tlist.append(12500)
  tlist.append(12750)
  tlist.append(13000)
  tlist.append(14000)
  tlist.append(15000)
  tlist.append(16000)
  tlist.append(17000)
  tlist.append(18000)
  tlist.append(19000)
  tlist.append(20000)
  tlist.append(21000)
  tlist.append(22000)
  tlist.append(23000)
  tlist.append(24000)
  tlist.append(25000)
  tlist.append(26000)
  tlist.append(27000)
  tlist.append(28000)
  tlist.append(29000)
  tlist.append(30000)
  tlist.append(31000)
  tlist.append(32000)
  tlist.append(33000)
  tlist.append(34000)
  tlist.append(35000)
  tlist.append(36000)
  tlist.append(37000)
  tlist.append(38000)
  tlist.append(39000)
  tlist.append(40000)
  tlist.append(41000)
  tlist.append(42000)
  tlist.append(43000)
  tlist.append(44000)
  tlist.append(45000)
  tlist.append(46000)
  tlist.append(47000)
  tlist.append(48000)
  tlist.append(49000)
  nlist = len(tlist)
  if verbose: print 'read Kurucz model file list'
  return nlist, tlist, flist

def read_filters(filters):
  "read filter response curve for each input filter"
  trans = []
  # first verify that I know what each filter is
  #   the process for adding new filters is described in /home/poseidon/sadams/impostors/dusty/NOTES
  #   but here's the outline:
  #     download filter response curve (WAVELENGTH THROUGHPUT o)
  #     add filter to MARCS/makefilter and run it (bash makefilter)
  #     update process.pl
  recognized_filters = np.array(['WFC3uvF275W','WFC3uvF336W','WFC3uvF438W','WFC3uvF475W','WFC3uvF555W','WFC3uvF606W','WFC3uvF814W','WFC3irF110W','WFC3irF160W','WFPC2F439W','WFPC2F450W','WFPC2F555W','WFPC2F675W','WFPC2F814W','ACSWFCF435W','ACSWFCF475W','ACSWFCF555W','ACSWFCF606W','ACSWFCF658N.dat','ACSWFCF814W','F36','F45','F58','F80','U','B','V','I','J','H','K','lbcrY','lbcrSDSSz','lbcrSDSSr','lbcrSDSSi','lbcrBesselV','lbcrBesselR','lbcrBesselI','lbcbSDSSr','lbcbSDSSg','lbcbUspec','lbcbBesselV','lbcbBesselU','lbcbBesselB','R','F24','F70','Ks','SW1','SW2','F34W','F46W','F12W','F22W','F7MIPS','F9MIPS','F10MIPS','F13MIPS','F14MIPS','F24A','F32A','F41A','sdssu','sdssg','sdssr','sdssi','sdssz','F240'])
  for filter in filters:
    match_filter = (recognized_filters == filter)
    if len(recognized_filters[match_filter]) == 1:
      filter_file = path_to_filter_files + 'filt' + recognized_filters[match_filter][0] + '.dat'
    else:
      print 'ERROR: unsupported filter: %s' % (filter)
      print '  this is the list of currently supported filters:'
      for sfilter in recognized_filters:
        print '\t\t%s' % (sfilter)
      exit(1)  
    # now read in the filter transmission curve
    templam, trans1 = np.loadtxt(filter_file, usecols=(0,1), unpack=True)
    trans.append(trans1)
  return templam, trans

def filter_effective_wavelength(filters):
  "return effective wavelengths for filters"
  lambdas = {}
  lambdas['U'] = 0.36	# microns
  lambdas['B'] = 0.44
  lambdas['V'] = 0.55
  lambdas['R'] = 0.658
  lambdas['sdssr'] = 0.612
  lambdas['sdssg'] = 0.464
  lambdas['WFC3uvF555W'] = 0.51874
  lambdas['WFC3uvF814W'] = 0.79011
  lambdas['WFC3irF110W'] = 1.15340
  lambdas['WFC3irF160W'] = 1.53690
  lambdas['lbcbBesselU'] = 0.36
  lambdas['lbcbBesselB'] = 0.44
  lambdas['lbcrBesselV'] = 0.55
  lambdas['lbcrBesselR'] = 0.658
  lambdas['lbcrBesselI'] = 0.806
  lambdas['lbcrBesselY'] = 1.03
  lambdas['F36'] = 3.6
  lambdas['F45'] = 4.5
  lambda_eff = []
  for filter in filters:
    try:
      lambda_eff.append(lambdas[filter])
    except:
      print 'ERROR: %s-band effective wavelength not found.  This must be added to the code.' % (filter)
      exit(1)
  return np.array(lambda_eff)

def add_header(outputfile):
  output = open(outputfile,'w')
  output.write("# Generated using '%s' at %s\n" % (''.join("%s " % ''.join(map(str, x)) for x in argv), datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
  output.write('# tstar = %s\n' % tstar)
  output.write('# ivarytstar = %s\n' % ivarytstar)
  output.write('# tau = %s\n' % tau)
  output.write('# ivarytau = %s\n' % ivarytau)
  output.write('# td = %s\n' % td)
  output.write('# ivarytd = %s\n' % ivarytd)
  output.write('# thick = %s\n' % thick)
  output.write('# ivarythick = %s\n' % ivarythick)
  output.write('# v0 = %s\n' % v0)
  output.write('# evlog = %s\n' % evlog)
  output.write('# vlog0 = %s\n' % vlog0)
  output.write('# tstart = %s\n' % tstart)
  output.write('# tnow = %s\n' % tnow)
  output.write('# dust = %s\n' % dust_type)
  output.write('# shell/wind model = %s\n' % shell_or_wind)
  output.write('# Metallicity = %s\n' % args.metallicity)
  output.write('# photometry_file = %s\n' % photometry_file)
  for i in xrange(len(mlam)):
    output.write('#   filters%d = %s %s %s %s\n' % (i, mlam[i], mlumobs[i], merrobs[i], filters[i]))
  output.write('# extrapolation = %s\n' % extrapolation)
  if extrapolation:
    output.write('#   textrapolate = %s' % textrapolate)
  output.write('# dLdt_prior = %s\n' % dLdt_limit)
  if dLdt_limit:
    output.write('#   dLdt_filter = %s\n' % args.dLdt_filters)
    output.write('#   dLdt_obs = %s\n' % args.dLdt_obs)
    output.write('#   dLdt_obs_err = %s\n' % args.dLdt_obs_errs)
  if custom_grain_distribution:
    output.write('# custom_grain_size_distribution = 1\n#   q = 3.5\n#   a_min = %s %% [micron]\n#   a_max = %s %% [micron]\n' % (amin,amax))
  else:
    output.write('# custom_grain_size_distribution = 0\n#   (standard MRN dust grain size distribution)\n#   q = 3.5\n#   a_min = 0.005 % [micron]\n#   a_max = 0.25 % [micron]\n')
  output.write('# ivarysize = %s\n' % args.ivarysize)
  output.write('# velocity constraint applied to r%d\n' % (args.dust_inout))
  output.write('# rescale_factor = %s\n' % (args.rescale_factor))
  if args.Lmax > 0: output.write('# Lmax = %s\n' % (args.Lmax))
  if args.fixLstar: output.write('# fixLstar = %s\n' % (args.fixLstar))
  if args.infer_flux:
    output.write('# chi         \ttstarnew    \ttaunew    \ttdnew     \tthicknew    \tsluml    \tr1      \tvlog	\tebv	\tamin	\tamax	\tRv\t\t%s\n' % ('\t\t'.join("%s " % ''.join(str(x)) for x in args.filt_infer))) 
  else:
    output.write('# chi\ttstarnew\ttaunew\ttdnew\tthicknew\tsluml\tr1\tvlog\tebv\tamin\tamax\tRv\n')
  output.close()


def geninput(tstar,tau,td,thick,ebv,idtype,fileuse):
  output = open('foo1.inp','w')
  if (tstar < 3500 or tstar > 48999) or (args.blackbody):
    output.write('Spectrum = 1\n')
    output.write('Number of BB = 1\n')
    output.write('Temperature = %0.2f\n' % (tstar))
  else:
    output.write('Spectrum = 5   \n')
    output.write('%s\n' % (fileuse))
  output.write('   optical properties index = 1 \n')
  output.write('   #   Sil-Ow  Sil-Oc  Sil-DL  grf-DL  amC-Hn  SiC-Pg \n')
  if idtype == 0:
    output.write('    x = 0.00    0.00   0.00    1.00    0.00    0.00 \n')
  else:
    output.write('    x = 0.00    0.00   1.00    0.00    0.00    0.00 \n')
  if custom_grain_distribution:
    output.write('- size distribution = 2  % custom       \n')
    output.write('  q = 3.5, a(min) = %s micron, a(max) = %s micron\n' % (aminnew,amaxnew))
  else:
    output.write('- size distribution = 1  % standard MRN    \n')
  output.write('- temperature = %s K \n' % (td))
  output.write('- density type = 1                   \n')
  output.write('- number of powers = 1              \n')
  output.write('- shells relative thickness = %s\n' % (thick))
  output.write('- power = 2 \n')
  output.write('- grid type = 1                  % linear grid \n')
  output.write('- lambda0 = 0.55 micron          % optical depth specified  \n')
  output.write('- tau(min) = '+str(tau)+' ; tau(max) = 1000.0   % for the visual wavelength \n')
  output.write('- number of models = 1           \n')
  output.write('- accuracy for flux conservation = 0.05             \n')
  output.write('- verbosity flag;                              verbose = 1  \n')
  output.write('- properties of emerging spectra;            fname.spp = 1  \n')
  output.write('- detailed spectra for each model;          fname.s### = 1  \n')
  output.write('- images at specified wavelengths;          fname.i### = 1  \n')
  output.write('     number of wavelengths = 5  \n')
  output.write('     wavelengths = 3.5, 4.5, 6.0, 8.0, 24.0 micron  \n')
  output.write('- radial profiles for each model;           fname.r### = 1  \n')
  output.write('- detailed run-time messages;               fname.m### = 1  \n')
  output.write('- visibility function at spec. wavelengths; fname.v### = 0  \n')
  output.close()

  if verbose: print 'calling dusty with tstar = %0.1f; tau = %0.2f; td = %0.1f; thick = %0.2f; amin = %0.3f amax = %0.3f; E(B-V) = %0.4f; R_V = %0.2f' % (tstar,tau,td,thick,aminnew,amaxnew,ebv,rvnew)
  system('./dusty')

  ierror = 0
  #try:
  data = ascii.read('foo1.stb')
  lam = data['col1']
  flx = data['col2']
  npt = len(lam)

  i = 0
  with open('foo1.out','r') as f:
    for line in f:
      if i == 42:
        # print 'line read in from foo1.out:', line
        line_s = line.split()
        id = int(line_s[0])
        tau0 = float(line_s[1])
        f1 = float(line_s[2])
        r1 = float(line_s[3])
        r1torstar = float(line_s[4])
        theta1 = float(line_s[5])
        tdout = float(line_s[6])
        break
      i += 1   
  #except:
  #  ierror = 1
  #  lam = np.nan
  #  flx = np.nan
  #  npt = np.nan
  #  r1 = np.nan
  return lam, flx, npt, r1, ierror

def locate(xx,n,x):
  jl = 0
  ju = n+1
  while ju - jl > 1:
    jm = (ju+jl) / 2
    if (xx[n-1] > xx[0] and x > xx[jm-1] ) or (xx[n-1] < xx[0] and x < xx[jm-1]):
      jl = jm
    else:
      ju = jm
  j = jl
  return j    

def date2jd(calendar_date):
  date_s = calendar_date.split('-')
  year = date_s[0]
  month = date_s[1]
  day = date_s[2]
  jd_date = jdcal.gcal2jd(year,month,day)[1]
  return jd_date

#======================================================================
# Initialize Parameters
#======================================================================

debug = 0

progenitor = 0  # if progenitor=1 use progenitor magnitude and write separate output
progenitor_photometry_file = 'premags.dat'		# name of file giving progenitor photometry
object_photometry_file = '2014_mags_detections.dat'	# name of file giving photometry of the object

tstar = 10000	# stellar temperature
ivarytstar = 1	# vary stellar temperature?

tau = 3		# tau
ivarytau = 1	# vary tau?

#td = 1500       # dust temperature
td = 250	 # dust temperature
ivarytd = 1	# vary dust temperature?

thick = 2.0	# thickness
ivarythick = 0	# vary thickness?

ebv = 0.0	# galaxy extinction E(B-V)
ivaryext = 0	# vary extinction

v0 = 765.0      # velocity [km/s]	# SN 1997bs according to Smith et al. 2011
#v0 = 560.0	# velocity [km/s]	# NGC300-OT according to Smith et al. 2011
#v0 = 1100.0	# velocity [km/s]	# SN 2008S according to Smith et al. 2011
evlog = 0.3	# log error

extrapolation = 0	# extrapolate model from a previously calculated date?
extrapolation_date = '2009-11-29'
extrapolation_name = '2009'

dLdt_limit = 0	# enforce a limit on dL/dt?
dLdt_filter = ['WFC3uvF555W'] # filter to apply dL/dt limit to
#dLdt_obs = -370 # [L_sun/yr]
#dLdt_obs_err = 180      # [L_sun/yr] (1-sigma uncertainty)
dLdt_obs = [320]	# [L_sun/yr]
dLdt_obs_err = [394]	# [L_sun/yr] (1-sigma uncertainty)

tstart_date = '1997-4-15'	# SN 1997bs
#tstart_date = '2008-4-24'	# NGC 300-OT (probably several days before this date...)
#tstart_date = '2008-2-2'	# SN2008S (Arbour & Boles 2008)
#tstart = jdcal.gcal2jd(1997,4,15)[1]	# date of peak
#tstart = 50553	# date of peak
#tnow = jdcal.gcal2jd(2014,1,3)[1]	# date of observation (avg of 2013/2014)
tnow_date = '2014-01-01'	# SN 1997bs observation
#tnow_date = '2014-8-29'	# NGC 300-OT Aug 2014 SST observation
#tnow_date = '2015-1-31' # latest SN2008S SST observations
#tnow = jdcal.gcal2jd(2013,11,29)[1]  # date of observation - 2013
#tnow = 56625	# date of observation  -- 2013
#tnow = 53369	# date of observation	-- 2004


idtype = 0	# dust type: 0=graphite 1=silicate
shell = 1	# shell/wind: 0=wind 1=shell

custom_grain_distribution = 0	# use a custom grain distribution? If 0 then use the standard MRN distribution
amin = 0.005 # micron
amax = 0.25   # micron
#amin = 0.035 # micron

ntrial = 10501	# number of iterations
continue_from_file = 0	# continue interupted MCMC chain from the last line of outputfile

#path_to_filter_files = '../MARCS/'
path_to_filter_files = '/scr/sma/software/dusty/MARCS/'

#----------------------------------------------------------------------
# Override hard-coded parameters with command line options

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-v", "--verbose", help="increase output verbosity", action="store_true") 
parser.add_argument("--veryverbose", help="increase output verbosity", action="store_true")
parser.add_argument("--tstar", help="stellar temperature", type=float, default=tstar)
parser.add_argument("--ivarytstar", help="vary stellar temperature?", type=int, choices=[0,1], default=ivarytstar)
parser.add_argument("--tau", help="tau", type=float, default=tau)
parser.add_argument("--ivarytau", help="vary tau?", type=int, choices=[0,1], default=ivarytau)
parser.add_argument("--td", help="dust temperature", type=float, default=td)
parser.add_argument("--ivarytd", help="vary dust temperature", type=int, choices=[0,1], default=ivarytd)
parser.add_argument("--thick", help="thickness", type=float, default=thick)
parser.add_argument("--ivarythick", help="vary thickness", type=int, choices=[0,1], default=ivarythick)
parser.add_argument("--v0", help="velocity", type=float, default=v0)
parser.add_argument("--evlog", help="log velocity err", type=float, default=evlog)
parser.add_argument("--extrapolation", help="extrapolate model from a previously calculated date?", type=int, choices=[0,1], default=extrapolation)
parser.add_argument("--extrapolation_date", help="date to extrapolate model to [yyyy-mm-dd] (if using extrapolation option)", type=str, default=extrapolation_date)
parser.add_argument("--extrapolation_name", help="output name for extrapolation model (if using extrapolation option)", type=str, default=extrapolation_name)
parser.add_argument("--dLdt_limit", help="use a limit on dL/dt?", type=int, choices=[0,1], default=dLdt_limit)
parser.add_argument("--dLdt_filters", help="list of filters to apply dL/dt limits to (if using dLdt_limit option)", nargs='+', type=str, default=dLdt_filter)
parser.add_argument("--dLdt_obs", help="observed dL/dt's [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float, default=dLdt_obs)
parser.add_argument("--dLdt_obs_errs", help="observed dL/dt 1-sigma uncertainties [L_sun/yr] (if using dLdt_limit option)", nargs='+', type=float, default=dLdt_obs_err)
parser.add_argument("--ref_lists", help="names of ref_list files used to generate reference images used by light curves", nargs="+")
parser.add_argument("--variability_constraints", help="use variability constraints from light curve(s)?", action="store_true")
parser.add_argument("--variability_files", help="list of files containing calibrated light curves (with 'L' and 'Lerr' in L_sun and 'DATE' in JD)", nargs="+")
parser.add_argument("--vardateoffset", help="days to subtract off lightcurve epochs to set to JD-2400000", type=float, choices=[0,2400000], default=0)
parser.add_argument("--min_years_elapsed", help="minimum years since event to start using lightcurve constraint", type=float, default=2.0)
parser.add_argument("--start_date", help="date shell ejected [yyyy-mm-dd]", type=str, default=tstart_date)
parser.add_argument("--now_date", help="date of observations [yyyy-mm-dd]", type=str, default=tnow_date)
parser.add_argument("--idtype", help="dust type: 0=graphite 1=silicate", type=int, choices=[0,1], default=idtype)
parser.add_argument("--shell", help="model type: 0=wind 1=shell", type=int, choices=[0,1], default=shell)
parser.add_argument("--custom_grain_distribution", help="use a custom grain distribution?", type=int, choices=[0,1], default=custom_grain_distribution)
parser.add_argument("--amin", help="minimum grain size [micron] (if using custom grain distribution)", type=float, default=amin)
parser.add_argument("--amax", help="maximum grain size [micron] (if using custom grain distribution)", type=float, default=amax)
parser.add_argument("--ivarysize", help="vary dust grain size?", type=int, choices=[0,1], default=0)
parser.add_argument("--ntrial", help="number of MCMC steps to write", type=int, default=ntrial)
parser.add_argument("--continue_from_file", help="continue interupted MCMC chain from the last line of outputfile?", default=continue_from_file)
parser.add_argument("--path_to_filter_files", help="path to filter transmission and stellar spectra files", type=str, default=path_to_filter_files)
parser.add_argument("--progenitor", help="use progenitor mode? (uses progenitor photometry file and tau=0 with tstar from the best-fit model found earlier when progenitor mode was disabled)", type=int, choices=[0,1], default=progenitor)
parser.add_argument("--progenitor_photometry_file", help="name of file giving progenitor photometry (used when progenitor mode enabled)", type=str, default=progenitor_photometry_file)
parser.add_argument("--object_photometry_file", help="name of file giving photometry of the object (used when progenitor mode disabled)", type=str, default=object_photometry_file)
#parser.add_argument("--effective_optical_depth", help="use the effective (rather than total) optical depth for the dL/dt chi^2 calculation?", choices=[0,1], default=1)
parser.add_argument("--effective_optical_depth_lambda", help="effective wavelength of dLdt_filter [micro-meters]", default=0.55, type=float)
parser.add_argument("--ivaryext", help="vary extinction", type=int, choices=[0,1], default=ivaryext)
parser.add_argument("--ebv", help="extinction E(B-V) from local galaxy", type=float, default=ebv)
parser.add_argument("--maxebv", help="max extinction E(B-V) from local galaxy", type=float, default=2.0)
parser.add_argument("--ivaryrv", help="vary extinction R_V", choices=[0,1], type=int)
parser.add_argument("--rv", help="initial R_V", type=float, default=3.1)
parser.add_argument("--minrv", help="minimum value for R_V", type=float, default=1.0)
parser.add_argument("--maxrv", help="maximum value for R_V", type=float, default=5.0)
parser.add_argument("--maxtau", help="Maximum tau to evaluate", type=float, default=1000.0)
parser.add_argument("--rescale_factor", help="Rescale MCMC step sizes by this factor (increasing it decreases the write fraction)", type=float, default=1.0)
parser.add_argument("--Lmax", help="Maximum stellar luminosity to accept [log L/L_sun].  Default: no maximum", type=float, default=-1)
parser.add_argument("--metallicity", help="Model atmosphere metallicity to use", choices=['m10','m05','p00','p02','p05'], default='p00')
parser.add_argument("--fixLstar", help="Force Lstar to a given luminosity [log L/L_sun]", type=float)
parser.add_argument("--minTstar", type=float, default=3500)
parser.add_argument("--maxTstar", type=float, default=48999)
parser.add_argument("--lim_only", help="Photometric constraint only upper limits?", action="store_true")
parser.add_argument("--chilim", help="chi^2 limit for 'lim_only' case", type=float, default=4.0)
parser.add_argument("--blackbody", help="Use blackbody instead of stellar atmosphere", action="store_true")
parser.add_argument("--dust_inout", help="apply velocity constraint to inner (1) or outer (2) dust radius?", choices=[1,2], default=2)
parser.add_argument("--infer_flux", help="infer fluxes through a set of filter band-passes?", action="store_true")
parser.add_argument("--filt_infer", help="list of filters for which to infer intrinsic fluxes", nargs="+", type=str, default=["sdssg","sdssr"])
args = parser.parse_args()

# now update parameters with possible changes from command line arguments
verbose = args.verbose
tstar = args.tstar
ivarytstar = args.ivarytstar
tau = args.tau
ivarytau = args.ivarytau
td = args.td
ivarytd = args.ivarytd
thick = args.thick
ivarythick = args.ivarythick
v0 = args.v0
evlog = args.evlog
extrapolation = args.extrapolation
extrapolation_name = args.extrapolation_name
dLdt_limit = args.dLdt_limit
#dLdt_filter = args.dLdt_filter
#dLdt_obs = args.dLdt_obs
#dLdt_obs_err = args.dLdt_obs_err
idtype = args.idtype
shell = args.shell
custom_grain_distribution = args.custom_grain_distribution
amin = args.amin
amax = args.amax
rv = args.rv
ntrial = args.ntrial
continue_from_file = args.continue_from_file
path_to_filter_files = args.path_to_filter_files
progenitor = args.progenitor
progenitor_photometry_file = args.progenitor_photometry_file
object_photometry_file = args.object_photometry_file
ebv = args.ebv
ivaryext = args.ivaryext
if args.ivarysize and (args.amin != args.amax):
  print 'Error: varying grain sizes is only currently implemented for a single size, not for a distribution\nPlease make amin = amax\n'
  exit(-1)

nfilt_infer = len(args.filt_infer)

textrapolate = date2jd(args.extrapolation_date)
tstart = date2jd(args.start_date) + args.vardateoffset
tnow = date2jd(args.now_date) + args.vardateoffset

vlog0 = np.log10(v0)
if progenitor:
  photometry_file = progenitor_photometry_file
else:
  photometry_file = object_photometry_file

#======================================================================

# If modeling progenitor then overwrite parameters with those used by the 'best' model
#   but change tau to zero
if progenitor or extrapolation:
  tstar, tau, td_best, thick, llum, lrad = np.loadtxt('best.dat',usecols=(1,2,3,4,5,6))
  if progenitor:
    ivarytstar, ivarytau, ivarytd, ivarythick = 0, 0, 0, 0
    ntrial = 1
  if extrapolation:
    ivarytstar, ivarytau, ivarythick = 0, 0, 0
  continue_from_file = 0
  header = return_ascii_starting_header('results.dat','#')
  shell_type = read_header(header,'dust')
  model_type = read_header(header,'shell/wind model')
  try:
    custom_grain_distribution = int(read_header(header,'custom_grain_size_distribution'))
    if custom_grain_distribution:
      amin = read_header(header,'a_min')
      amax = read_header(header,'a_max')
  except:
    custom_grain_distribution = 0
  tstart = float(read_header(header,'tstart'))
  tnow = float(read_header(header,'tnow'))
  if shell_type == 'silicate':
    idtype = 1
  elif shell_type == 'graphite':
    idtype = 0
  else:
    print 'ERROR: unrecognized dust type "%s" in %s' % (shell_type,'results.dat')
  if model_type == 'shell':
    shell = 1
  elif model_type == 'wind':
    shell = 0
  else:
    print 'ERROR: unrecognized model type "%s" in %s' % (model_type,'results.dat')
    exit(1)
if progenitor:
  bestfile = 'prog'
  outputfile = 'prog_results.dat'
  alloutputfile = 'prog_allresults.dat'
  tau = 0
  td = td_best
elif extrapolation:
  bestfile = 'extrapolation'+extrapolation_name
  outputfile = 'extrapolation'+extrapolation_name+'_results.dat'
  alloutputfile = 'extrapolation'+extrapolation_name+'_allresults.dat'
  tau_orig = tau
  t_orig = tnow
  tau = tau_orig*((textrapolate-tstart)/(t_orig-tstart))**(-2)
  tnow = textrapolate
  if verbose: print '  extrapolating tau=%0.2f at t_elap = %d [days]\n    to tau=%0.2f at t_elap = %d [days]\n    according to tau propto t^-2' % (tau_orig,t_orig-tstart, tau, textrapolate-tstart)
else:
  bestfile = 'best'
  outputfile = 'results.dat'
  alloutputfile = 'allresults.dat'

telapsed = tnow-tstart
telapsed_years = telapsed / 365.25

if verbose: print 'elapsed time %s days' % (telapsed)
# coefficient 8.64e09 is (1 km/s)(day) so dlog = log(distance moved given time baseline at 1km/s)
dlog = np.log10(8.640e9*telapsed)
if verbose: print 'dlog = %0.2f' % (dlog)

# Note: I mistakenly had the dust_type labels reversed prior to 6 Nov 2014
if idtype == 1:
  dust_type = 'silicate'
else:
  dust_type = 'graphite'

if shell == 1:
  shell_or_wind = 'shell'
else:
  shell_or_wind = 'wind'

#if dLdt_limit and shell_or_wind == 'wind':
#  print "um, it doesn't make sense to apply a dL/dt limit for the steady-wind scenario... quiting"
#  exit(1)

# read in the stellar spectra
nlist, tlist, flist = readkurucz()
marcsflux = np.zeros([nlist,417]) # I'm assuming all of the spectra files have 417 lines
for i in xrange(nlist):
  marcslam, marcsflux1 = np.loadtxt(flist[i], usecols=(0,1), unpack=True)
  try:
    marcsflux[i,:] = marcsflux1
  except:
    print 'ERROR: this program is hard-coded to expect spectra file with 417 lines only'
    print '  %s has %d lines' % (flist[i], len(marcsflux1))
    exit(1)
if verbose: print 'done reading in model spectra'

# read in the photometry file (magnitudes, filters)
data = ascii.read(photometry_file)
mlam = data['col1']
mlumobs = data['col2']
#mlumlobs = np.zeros(len(mlumobs))
#merrlobs = np.zeros(len(mlumobs))
#gtzero = (mlumobs > 0)
#ltzero = np.invert(gtzero)
#mlumlobs[gtzero] = np.log10(mlumobs[gtzero])
merrobs = data['col3']
#merrlobs[gtzero] = merrobs[gtzero]/mlumobs[gtzero]/np.log(10)
#merrlobs[ltzero] = np.log10(merrobs[ltzero])
filters = data['col4']
#templam, trans = read_filters(filters)
nm = np.shape(mlumobs)[0]
if verbose: print 'read %d data points' % (nm)


#----------------------------------------------------------------------

# 8 Mar 2015
# I still need to modify this section to include extinction
if continue_from_file:
  bestdata = ascii.read(bestfile+'.dat')
  try:
    chibest = bestdata['chi'][0]
    tstarbest = bestdata['tstar'][0]
    taubest = bestdata['tau'][0]
    tdbest = bestdata['td'][0]
    thickbest = bestdata['thick'][0]
    slumlbest = bestdata['sluml'][0]
    r1best = bestdata['r1'][0]
    ebvbest = bestdata['ebv'][0]
    aminbest = bestdata['amin'][0]
    amaxbest = bestdata['amax'][0]
    rvbest = bestdata['Rv'][0]
  except:
    try:
      chibest = bestdata['col1'][0]
      tstarbest = bestdata['col2'][0]
      taubest = bestdata['col3'][0]
      tdbest = bestdata['col4'][0]
      thickbest = bestdata['col5'][0]
      slumlbest = bestdata['col6'][0]
      r1best = bestdata['col7'][0]
      ebvbest = bestdata['col8'][0]
      aminbest = bestdata['col9'][0]
      amaxbest = bestdata['col10'][0]
      rvbest = bestdata['col11'][0]
    except:
      print 'Error reading %s' % (bestfile+'.dat')
      exit(1)
  #Vlumbest = -1
  # get initial tstar,tau,td,thick from last line of output file
  #   it is the user's responsibility to make sure that the other
  #   parameters (e.g., ivarytstar) are set appropriately
  #   -> I could make the code get this from the output file's header...
  import subprocess
  process = subprocess.Popen(["tail","--line","1",outputfile], stderr=subprocess.STDOUT, stdout=subprocess.PIPE)
  output, err = process.communicate()
  output_s = output.split()
  chiold = float(output_s[0])
  tstar = float(output_s[1])
  tau = float(output_s[2])
  td = float(output_s[3])
  thick = float(output_s[4])  
  ebv = float(output_s[8])
  amin = float(output_s[9])
  amax = float(output_s[10])
  rv = float(output_s[11])
  tmp = np.loadtxt(outputfile,comments='#')
  linesprior = float(len(tmp))
  print '%d lines realizations written previously' % linesprior
else:
  chiold = 1.e32
  chibest = 1.e32
  tstarbest = 1.e32
  taubest = 1.e32
  tdbest = 1.e32
  thickbest = 1.e32
  slumlbest = 1.e32
  r1best = 1.e32
  ebvbest = 1.e32
  aminbest = 1.e32
  amaxbest = 1.e32
  rvbest = 1.e32
  add_header(outputfile)
  add_header(alloutputfile)
  linesprior = 0

#----------------------------------------------------------------------
# Other one-time things
if dLdt_limit or args.variability_constraints:
  nvar = len(args.dLdt_filters)
  dLdt_lambdas = filter_effective_wavelength(args.dLdt_filters)
  if (ebv != 0 or ivaryext) and args.variability_constraints:
    print 'ERROR: I have not yet implemented extinction correction for variability constraints'
    exit(1)

if args.infer_flux:
  filt_infer_lambdas = filter_effective_wavelength(args.filt_infer)
  print 'INFER_FLUX MODE ON'
  print 'note: inferred fluxes currently only tested for tau=0 and E(B-V)=0 case'



ltstar = np.log10(tstar)
ltau = np.log10(tau+1.e-10)	# avoid error if given tau=0
ltd = np.log10(td)
lthick = np.log10(thick)
if args.ivaryext: 
  if ebv < 0.001: lebv = -3.0
  else: lebv = np.log10(ebv)
else: lebv = np.log10(ebv+1.e-10)	# avoid error if given extinction=0
lmaxebv = np.log10(args.maxebv)
ebvstepcoeff = (lmaxebv + 3) / 17.0
eweight = 1.0

#----------------------------------------------------------------------
# Read in Variability data

if args.variability_constraints:
  lightcurve_data = {}
  ref_data = {}
  for i in xrange(len(args.dLdt_filters)):
    filter = args.dLdt_filters[i]
    lightcurve_data[filter] = ascii.read(args.variability_files[i],delimiter=',')
    lightcurve_data[filter]['L'] = lightcurve_data[filter]['L'] * -1 	# ISIS has the sign backwards...
    lightcurve_data[filter]['YEARS_ELAPSED'] = (lightcurve_data[filter]['DATE'] - tstart) / 365.25
    lightcurve_data[filter]['years_elapsed_mask'] = (lightcurve_data[filter]['YEARS_ELAPSED'] > args.min_years_elapsed)
    if verbose: print 'lightcurve points before trim:', len(lightcurve_data[filter]['YEARS_ELAPSED'])
    if verbose: print 'lightcurve points after trim:', len(lightcurve_data[filter]['YEARS_ELAPSED'][lightcurve_data[filter]['years_elapsed_mask']])
    # read in ref_list's -- images used to construct the reference images    
    ref_data[filter+'_REF_DATES'] = np.loadtxt(args.ref_lists[i], usecols=(1,2), unpack=True)[0]
    ref_data[filter+'_REF_YEARS_ELAPSED'] = (ref_data[filter+'_REF_DATES'] - tstart) / 365.25
    if verbose: print 'reference years elapsed:', ref_data[filter+'_REF_YEARS_ELAPSED']
    #lightcurve_data[filter]['REF_MEAN_YEARS_ELAPSED'] = np.average(lightcurve_data[filter]['REF_YEARS_ELAPSED'])
    mask = (ref_data[filter+'_REF_YEARS_ELAPSED'] > 0)
    fpostevent = float(len(ref_data[filter+'_REF_YEARS_ELAPSED'][mask])) / float(len(ref_data[filter+'_REF_YEARS_ELAPSED']))
    mask = (ref_data[filter+'_REF_YEARS_ELAPSED'] > args.min_years_elapsed)
    fpostminyearselapsed = float(len(ref_data[filter+'_REF_YEARS_ELAPSED'][mask])) / float(len(ref_data[filter+'_REF_YEARS_ELAPSED']))
    if fpostevent == 1 and fpostminyearselapsed == 1:
      ref_post_transient = 1
    elif fpostevent == 0:
      ref_post_transient = 0
      print 'WARNING: ref from pre-event imaging -- this scenario has not been tested yet...'
    else:
      print 'ERROR: %s is a mixture of pre and post event imaging -- this is not currently supported' % (args.ref_lists[i])
      exit(1)
    if verbose: print 'ref_post_transient =', ref_post_transient

#----------------------------------------------------------------------
#mintauscaled = 1.0 / (args.maxtau + 1.0)
#maxtauscaled = 1.0
mintauscaled = 0.0
maxtauscaled = np.log10(args.maxtau+1.0)
minlogTstar = np.log10(args.minTstar)
maxlogTstar = np.log10(args.maxTstar)

#for k in xrange(ntrial):
n = 0 
if continue_from_file:
  k = linesprior
else:
  k = 0
while k < ntrial + 1:
  n += 1
  if verbose:
    print '\n\ndoing realization #%d' % (n)
    print '  %d MCMC steps written thus far' % (k-linesprior)
  # extinction
  if ivaryext == 1:
    lebvnew = 0
    while lebvnew == 0:
      #possible_lebvnew = lebv + args.rescale_factor*0.15*gasdev(0,1,1)[0]
      possible_lebvnew = lebv + args.rescale_factor*ebvstepcoeff*gasdev(0,1,1)[0]
      if (possible_lebvnew > -3) and (possible_lebvnew < lmaxebv):
        lebvnew = possible_lebvnew
  else: lebvnew = lebv

  # stellar temperature
  if ivarytstar == 1:
    ltstarnew = 0
    while ltstarnew == 0:
      possible_ltstarnew = ltstar + args.rescale_factor*0.07*gasdev(0,1,1)[0]    # coefficient used for updated 9 runs
      #possible_ltstarnew = ltstar + 0.1*gasdev(0,1,1)[0]    # coefficient used for updated 7 & 8 runs
      #possible_ltstarnew = ltstar + 0.21*gasdev(0,1,1)[0]
      #possible_ltstarnew = ltstar + 0.05*gasdev(0,1,1)[0]
      # restrict temperature to 3500 < T < 49000
      #if (possible_ltstarnew > 3.398) and (possible_ltstarnew < 4.477):
      #if (possible_ltstarnew > 3.544) and (possible_ltstarnew < 4.591):
      #if (possible_ltstarnew > 3.544) and (possible_ltstarnew < 4.690):
      if (possible_ltstarnew > minlogTstar) and (possible_ltstarnew < maxlogTstar):
        ltstarnew = possible_ltstarnew
  else: ltstarnew = ltstar

  # optical depth
  if ivarytau == 1:
    ltaunew = 0
    #tauscaled = 1.0 / (10**ltau+1)
    tauscaled = np.log10(10**ltau+1.0)
    while ltaunew == 0:
      #possible_ltaunew = ltau + 0.3*gasdev(0,1,1)[0]
      #possible_ltaunew = ltau + 0.05*gasdev(0,1,1)[0]
      # restrict optical depth to tau < 1000
      #if possible_ltaunew < 3.0: ltaunew = possible_ltaunew
      #if possible_ltaunew < 3.0 and possible_ltaunew > -3.0: ltaunew = possible_ltaunew
      # really we want a log prior for large tau (>1) and a linear prior for small tau (<1)
      possible_tauscalednew = tauscaled + args.rescale_factor*0.12*gasdev(0,1,1)[0]    # coefficient used for updated 9 runs
      #possible_tauscalednew = tauscaled + 0.04*gasdev(0,1,1)[0]    # coefficient used for updated 7 & 8 runs
      #if possible_tauscalednew > 1.0e-3:
      if possible_tauscalednew > mintauscaled and possible_tauscalednew < maxtauscaled:
        #print '*** tau draws needed = %d ***' % z
        #ltaunew = np.log10(1.0/possible_tauscalednew - 1.0)
        ltaunew = np.log10(10**possible_tauscalednew-1.0)
  else: ltaunew = ltau

  # dust temperature
  if ivarytd == 1:
    ltdnew = 0
    while ltdnew == 0:
      possible_ltdnew = ltd + args.rescale_factor*0.1*gasdev(0,1,1)[0]     # coefficient used for updated 9 runs
      #possible_ltdnew = ltd + 0.18*gasdev(0,1,1)[0]	# coefficient used for updated 7 & 8 runs
      #possible_ltdnew = ltd + 0.4*gasdev(0,1,1)[0]
      #possible_ltdnew = ltd + 0.07*gasdev(0,1,1)[0]
      # restrict dust temperature to 40 < T_dust < 2000
      if possible_ltdnew < 3.3 and possible_ltdnew > 1.602:
        ltdnew = possible_ltdnew
  else: ltdnew = ltd

  # thickness
  if ivarythick == 1:
    lthicknew = 0
    while lthicknew == 0:
      possible_lthicknew = lthick + args.rescale_factor*0.1*gasdev(0,1,1)[0]
      # restrict the thickness to 0.1 to 100
      if possible_lthicknew > 0.0414 and possible_lthicknew < 2.0:
        lthicknew = possible_lthicknew
  else: lthicknew = lthick

  # grain size
  if args.ivarysize == 1:
    aminnew = 0
    lamin = np.log10(amin)
    while aminnew == 0:
      possible_laminnew = lamin + args.rescale_factor*0.15*gasdev(0,1,1)[0]
      # restrict the grain size to 0.001 to 2
      if possible_laminnew > -3.0 and possible_laminnew < 0.3:
        aminnew = 10**possible_laminnew
    amaxnew = aminnew
  else:
    aminnew = amin
    amaxnew = amax

  # Dust R_V
  if args.ivaryrv == 1:
    rvnew = 0
    while rvnew == 0:
      possible_rvnew = rv + args.rescale_factor*0.1*gasdev(0,1,1)[0]
      # restrict R_V to 2-4
      if possible_rvnew > args.minrv and possible_rvnew < args.maxrv:
        rvnew = possible_rvnew
  else: rvnew = rv

  ebvnew = 10.0**lebvnew
  tstarnew = 10.0**ltstarnew
  if tstarnew == int(tstarnew): tstarnew += 0.001
  taunew = 10.0**ltaunew
  tdnew = 10.0**ltdnew
  thicknew = 10.0**lthicknew
  if verbose: print 'tdust = %0.1f' % (tdnew)

  # identify the input stellar spectrum closest to the requested spectrum
  if not ((tstarnew < 3500 or tstarnew > 48999) or (args.blackbody)):
    dmin = 1.0e32
    d = abs(10**ltstarnew-tlist)
    imin = np.argmin(d)
    if verbose: print 'will use temperature %s' % (tlist[imin])
    fileuse = 'tempspec.dat'
  
    # for some reason Kochanek's code takes the closest spectral template that 
    #   is cooler than the star instead of simply the closest overall
    # I'll try following here to see if this accounts for the differences
    #   in results between our codes
    d = 10**ltstarnew-tlist
    mask = (d<0)
    d[mask] = d[mask]*(-1e6)
    imin = np.argmin(d)
  
    # generate the stellar spectrum -- writes it to a temporary file (tempspec.dat)
    # which is then used by DUSTY as the input spectrum
    # keep it in the available range
    tstarnew = max(tlist[0],min(tlist[nlist-1],tstarnew))
    output = open(fileuse,'w')
    slope = (tstarnew-tlist[imin]) / (tlist[imin+1]-tlist[imin])	# this will probably crap out if the matched spectra is the last one...
    if debug: print 'slope = %s, %s %s %s' % (slope,tstarnew,tlist[imin],tlist[imin+1])
    if slope < 0 or slope > 1:
      print 'slope out of range: %f' % (slope)
      print 'wanted %0.1f' % (tstarnew)
      print 'bracket claim is %s,%s' % (tlist[imin],tlist[imin+1])
      exit(1)
    # write out the model spectrum being used
    #  NOTE: Kochanek's dustymc_detection.f cuts the spectra off after 200 lines
    #    but I'll use the whole spectrum because that just seems prudent
    for i in xrange(len(marcslam)):
      output.write('%g\t%g\n' % (marcslam[i], (1.0-slope)*marcsflux[imin,i] + slope*marcsflux[imin+1,i]))
    output.close()
  else:
    fileuse = ''
    if verbose: print 'using %0.1f blackbody' % (tstarnew)
  lam, flx, npt, r1, ierror = geninput(tstarnew,taunew,tdnew,thicknew,ebvnew,idtype,fileuse)

  if ierror == 1:
    if verbose: print 'had error, trying new model'
    continue

  # convert magnitudes to take out effect of extinction
  # mlum mlam lebvnew
  # ecor = 10.0**(-0.4*rval*ebvnew)
  mlum = np.zeros(len(mlumobs))
  merr = np.zeros(len(mlumobs))
  if args.veryverbose: print 'Extinction-corrected luminosities:'
  for i in xrange(len(mlam)):
    rlval = rl(rv,1.0/mlam[i])
    ecor = 10.0**(-0.4*rlval*ebvnew)
    mlum[i] = mlumobs[i] / ecor
    merr[i] = merrobs[i] / ecor
    if args.veryverbose: print '    %s %s %s %s (%s)' % (mlam[i], mlum[i], merr[i], filters[i], ecor)
  gtzero = (mlum > 0)
  ltzero = np.invert(gtzero)
  mluml = np.zeros(len(mlumobs))
  merrl = np.zeros(len(mlumobs))
  mluml[gtzero] = np.log10(mlum[gtzero])
  merrl[gtzero] = merr[gtzero]/mlum[gtzero]/np.log(10) 
  merrl[ltzero] = np.log10(merr[ltzero])

  # find the effective tau coefficient tau_e,lambda = sqrt(1 - albedo_lambda)*tau_lambda = tau_eff_coeff * tau_lambda
  #   and calculate tau ratios (tau_eff,lambda/tau_eff,V) seen by filters in which dL/dt measurements were made
  if (k == 0 or (continue_from_file and int(linesprior) == int(k))) and (args.variability_constraints or dLdt_limit):
    #print 'I am running the looop'
    tau_coeff, tau_ratios = calculate_tau_coeff()
    if verbose: print 'tau_ratios =', tau_ratios
    if k == 0:
      with open(outputfile,'a') as output:
        output.write('# tau_eff_coeff = %0.4f %% calculated from albedo for %0.4f micro-meters\n' % (tau_coeff, args.effective_optical_depth_lambda))
        for i in xrange(nvar):
          output.write('# tau_eff,%s / tau_eff,V = %0.2f\n' % (args.dLdt_filters[i],tau_ratios[i]))
      output.close()
  # loop through filters with luminosity constraints
  aa = 0.0
  bb = 0.0
  val = np.zeros(100)
  vall = np.zeros(100)
  for i in xrange(nm):
    j = locate(lam,npt,mlam[i]) - 1	# -1 because indexed to 0
    val[i] = flx[j] + (flx[j+1]-flx[j])*(mlam[i]-lam[j])/(lam[j+1]-lam[j])
    if debug: print 'i,j ',i,j,flx[j],flx[j+1],lam[j],lam[j+1],mlam[i],val[i]
    #print 'val[%s] = %s' % (i,val[i])
    # This is the luminosity chi^2 minimization
    if args.lim_only:
      aa = aa + (val[i]/merr[i])**2
      if mlum[i] > 0:
        print 'ERROR: I am expecting only upper limits'
        exit(1)
    else:
      vall[i] = np.log10(val[i]+1.e-32)
      if debug: print 'vall[%s] = %s' % (i,vall[i])
      if mlum[i] > 0:
        aa = aa + (mluml[i]-vall[i])/merrl[i]**2
        bb = bb + 1.0/merrl[i]**2
  if args.lim_only:
    slum = np.sqrt(args.chilim/aa)
    sluml = np.log10(slum)
  else:
    sluml = aa/bb
  if extrapolation:
    sluml = llum	# if extrapolating tau from a best-fit model then use the same luminosity
  if args.fixLstar:
    if verbose: print "forcing Lstar to 10^%0.2f" % (args.fixLstar)
    sluml = args.fixLstar
  # r1 output from DUSTY is the distance at which a point source w/luminosity 10^4 L_sun produces the bolometric flux F_e1
  r1 = np.log10(r1) + 0.5*(sluml-4.0)	# scale radius for luminosity
  r2 = r1 + np.log10(thicknew)
  # dusty reports a radius scaled to a luminosity of 10^4
  # so this scales it to the luminosity you just worked out
  chi = 0.0
  chis = {}
  if not extrapolation and not args.lim_only:
    for i in xrange(nm):
      if mlum[i] > 0: 
        chis[filters[i]] = ((mluml[i]-sluml-vall[i])/merrl[i])**2
        chi = chi + chis[filters[i]]
        #chi = chi + ((mluml[i]-sluml-vall[i])/merrl[i])**2
        if debug: print '%s = chi + ((%s-%s-%s)/%s)**2' % (chi,mluml[i],sluml,vall[i],merrl[i])
      if mlum[i] < 0:
        rat = 10.0**(sluml+vall[i]-merrl[i])
        if debug: print 'rat = %s = 10**(%s+%s-%s)' % (rat,sluml,vall[i],merrl[i])
        chis[filters[i]] = rat*rat
        chi = chi + chis[filters[i]]
        #chi = chi + rat*rat
  chi_Lobs = chi
  if verbose: print 'chi^2 from L_obs = %0.1f' % (chi_Lobs)
  chi = chi/eweight**2
  chi_dLdt = 0

  # infer fluxes in various filter band passes
  inferfilterlum = {}
  if args.infer_flux:
    for i in xrange(nfilt_infer):
      filter = args.filt_infer[i]
      j = locate(lam,npt,filt_infer_lambdas[i]) - 1
      valinfer = flx[j] + (flx[j+1]-flx[j])*(filt_infer_lambdas[i]-lam[j])/(lam[j+1]-lam[j])
      inferfilterlum[filter] = 10**sluml * valinfer

  if (dLdt_limit or args.variability_constraints) and not extrapolation:
    tau_eff = taunew*tau_coeff
    if verbose: print 'tau_tot = %0.3f\ntau_eff = %0.3f' % (taunew, tau_eff)
    # loop through filters with variability constraints
    for i in xrange(nvar):
      filter = args.dLdt_filters[i]
      j = locate(lam,npt,dLdt_lambdas[i]) - 1
      valvar = flx[j] + (flx[j+1]-flx[j])*(dLdt_lambdas[i]-lam[j])/(lam[j+1]-lam[j])
      varfilterlum = 10**sluml * valvar	# luminosity in a filter that has a dL/dt measurement
      tau_eff_filter = tau_eff*tau_ratios[i]
      if dLdt_limit:
        rlval_dLdt = rl(rv,1.0/dLdt_lambdas[i])
	dLdt_ecor = 10.0**(-0.4*rlval_dLdt*ebvnew)
        dLdt_model = 2.0*varfilterlum*tau_eff_filter / telapsed_years * dLdt_ecor
	#print 'dLdt_model before ext cor=%0.1d' % (dLdt_model/dLdt_ecor)
	#print 'dLdt_model after ext cor=%0.1d' % (dLdt_model)
        chis['dL_('+args.dLdt_filters[i]+')/dt'] = ( (dLdt_model - args.dLdt_obs[i]) / (args.dLdt_obs_errs[i]*dLdt_ecor) )**2
        if verbose: 
          print '***  %s L_modeled = %0.3g' % (args.dLdt_filters[i],varfilterlum)
          print '***    dL_(%s)/dt observed = %0.3g +/- %0.3g L_sun' % (args.dLdt_filters[i],args.dLdt_obs[i],args.dLdt_obs_errs[i])
          print '***    dL_(%s)/dt model = %0.3g L_sun' % (args.dLdt_filters[i],dLdt_model)
          print '***    chi^2_(dL/dt) = %0.3g' % (chis['dL_('+args.dLdt_filters[i]+')/dt'])
        #chi_dLdt = chi_dLdt + chis['dL_('+args.dLdt_filters[i]+')/dt']

      else:
        # luminosity evolves as L_1 = L_0 e^{-tau[(t_0/t_1)^2-]}
        # luminosity of epochs going into the reference image
        if ref_post_transient:
          modelreflums = varfilterlum * np.exp( -tau_eff_filter * ( (telapsed_years/ref_data[args.dLdt_filters[i]+'_REF_YEARS_ELAPSED'])**2 - 1.0 ) )
        else:
          modelreflums = varfilterlum * np.exp( tau_eff_filter ) # if the ref is from pre-transient I should use the
	  #	unobscured luminosity --> this is only in a testing stage...
        # predicted luminosity of the target in the reference image
        modelmean = np.average(modelreflums)
        # predictived luminosity of the target in all of the epochs making up the lightcurve data
        modellums = varfilterlum * np.exp( -tau_eff_filter * ( (telapsed_years/lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][lightcurve_data[filter]['years_elapsed_mask']])**2 - 1.0 ) )
        modeldiff = modellums - modelmean
        chi_epochs = ( (lightcurve_data[args.dLdt_filters[i]]['L'][lightcurve_data[filter]['years_elapsed_mask']] - modeldiff) / lightcurve_data[args.dLdt_filters[i]]['Lerr'][lightcurve_data[filter]['years_elapsed_mask']] )**2
        chis['dL_('+args.dLdt_filters[i]+')/dt'] = np.sum( chi_epochs )
        tauepoch = tau_eff_filter * (telapsed_years/lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][lightcurve_data[filter]['years_elapsed_mask']])**2
        if verbose:
          print '*** %s-band Variability Constraints' % (args.dLdt_filters[i])
          print '  %s L_modeled,now = %0.3g\twith tau_eff,%s = %0.2f' % (args.dLdt_filters[i],varfilterlum,args.dLdt_filters[i],tau_eff_filter)
          print '  %6s\t%11s\t%11s\t%11s\t%10s\t%10s' % ('YRS_ELAP','tau_eff,'+args.dLdt_filters[i],'delta_L_MOD','delta_L_OBS','LERR_OBS','CHI^2')
          for j in xrange(len(lightcurve_data[args.dLdt_filters[i]]['L'][lightcurve_data[filter]['years_elapsed_mask']])):
            print '  %6.2f\t%11.2f\t%11.2f\t%11.2f\t%10.2f\t%10.2f' % (lightcurve_data[args.dLdt_filters[i]]['YEARS_ELAPSED'][lightcurve_data[filter]['years_elapsed_mask']][j], tauepoch[j], modeldiff[j], lightcurve_data[args.dLdt_filters[i]]['L'][lightcurve_data[filter]['years_elapsed_mask']][j], lightcurve_data[args.dLdt_filters[i]]['Lerr'][lightcurve_data[filter]['years_elapsed_mask']][j], chi_epochs[j])
      chi_dLdt = chi_dLdt + chis['dL_('+args.dLdt_filters[i]+')/dt']

  if verbose: print 'chi^2 from dL/dt = %0.1f' % (chi_dLdt)
  chi = chi + chi_dLdt
  # prior on velocity
  if args.dust_inout == 1:
    dustdist = r1
  else:
    dustdist = r2
  vlog = dustdist-dlog 
  #chi = chi + ((vlog-vlog0)/evlog)**2
  if shell:
    chis['vlog'] = ((vlog-vlog0)/evlog)**2
    if verbose: print 'chi^2 from velocity = %0.5f for Tdust = %0.3f K' % (chis['vlog'],tdnew)
    r_modeled = 10**dustdist
    r_actual = 10**vlog0 * 8.640e9 * telapsed
    if verbose: print 'modeled r%d = %0.3g, actual r%d = %0.3g, diff = %0.3g for Tdust = %0.3f K' % (args.dust_inout, r_modeled, args.dust_inout, r_actual, r_actual-r_modeled, tdnew)
    if verbose:
      if r_actual-r_modeled > 0: print '  try decreasing t_dust'
      else: print '  try increasing t_dust'
  else:
    chis['vlog'] = 0
  chi = chi + chis['vlog']
  if verbose: print 'sluml = %0.1f; newchi = %0.1f; oldchi = %0.1f' % (sluml, chi, chiold)
  if chi < chibest:
    #if dLdt_limit: Vlumbest  = Vlum
    chibest   = chi
    tstarbest = tstarnew
    taubest   = taunew
    tdbest    = tdnew
    thickbest = thicknew
    slumlbest = sluml
    r1best    = r1
    ebvbest   = ebvnew
    aminbest  = aminnew
    amaxbest  = amaxnew
    rvbest    = rvnew
    #system('../copybest')
    system('cp foo1.inp '+bestfile+'.inp')
    system('cp foo1.itb '+bestfile+'.itb')
    system('cp foo1.out '+bestfile+'.out')
    system('cp foo1.rtb '+bestfile+'.rtb')
    system('cp foo1.spp '+bestfile+'.spp')
    system('cp foo1.stb '+bestfile+'.stb')
    output = open(bestfile+'.dat','w')
    outputsed = open(bestfile+'_ecor_mags.dat','w')
    if args.infer_flux: 
      output.write('chi         \ttstar       \ttau       \ttd        \tthick       \tsluml    \tr1      \tebv      \tamin      \tamax       \tRv\t\t%s\n' % ('\t\t'.join("%s " % ''.join(str(x)) for x in args.filt_infer)))
      output.write('%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%s\n' % (chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,ebvnew,aminnew,amaxnew,rvnew,'\t'.join("%s " % ''.join(str(inferfilterlum[str(x)])) for x in args.filt_infer)))
    else: 
      output.write('chi         \ttstar       \ttau       \ttd        \tthick       \tsluml    \tr1      \tebv      \tamin      \tamax       \tRv\n')
      output.write('%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\n' % (chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,ebvnew,aminnew,amaxnew,rvnew))
    for x in xrange(len(mlam)):
      outputsed.write('%s %s %s %s\n' % (mlam[x], mlum[x], merr[x], filters[x]))
    output.close()
    outputsed.close()
  if k != 0:
    keep = 0
    if chi < chiold:
      keep = 1
      if verbose: print '  keeping model case 1'
    else:
      prob = np.exp(-0.5*(chi-chiold))
      if rand() <= prob:
        keep = 1
        if verbose: print '  keeping model case 2. Prob = %0.2f' % (prob)
      else:
        if verbose: print '  rejecting model case 2. Prob = %0.2f' % (prob)
    # don't accept step if luminosity exceeds the maximum allowed
    #   (this is to prevent run-away solutions luminous, cold shells when not utilizing a velocity prior)
    if args.Lmax > 0:
      if sluml > args.Lmax:
        if verbose: print 'Luminosity exceeds Lmax set by user.  Rejecting model.'
        keep = 0
    if keep == 1:
      ltstar = ltstarnew
      ltau   = ltaunew
      ltd    = ltdnew
      lthick = lthicknew
      chiold = chi
      lebv    = lebvnew  
      amin    = aminnew
      amax    = amaxnew
      rv      = rvnew
      # indentation fix -- was screwed up from mid-March to 30 March 2015
      with open(outputfile,'a') as output:
        k += 1
        if args.infer_flux: output.write('%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%s\n' % (chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,vlog,ebvnew,amin,amax,rv,'\t'.join("%s " % ''.join(str(inferfilterlum[str(x)])) for x in args.filt_infer)))
        else: output.write('%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\t%f\n' % (chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,vlog,ebvnew,amin,amax,rv))
        output.close()
      #---- end indentation fix
    with open(alloutputfile,'a') as alloutput:
      alloutput.write('%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f,%s,%d\n' % (chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,vlog,ebvnew,amin,amax,rv,','.join("%s" % ''.join(str(x)) for x in chis.values()),keep))
    alloutput.close()
  else:
    k += 1
    if continue_from_file == 0:
      #with open(outputfile,'a') as output:
      #  output.write('chi,tstarnew,taunew,tdnew,thicknew,sluml,r1,vlog\n')
      #output.close()
      with open(alloutputfile,'a') as alloutput:
        alloutput.write('chi,tstar,tau,td,thick,sluml,r1,vlog,ebv,amin,amax,Rv,%s,keep\n' % (','.join("chi_%s" % ''.join(map(str, x)) for x in chis.keys())))
      alloutput.close()
    chiold = chi

  if verbose: 
    print 'best model chi^2 = %0.1f' % (chibest)
    #if dLdt_limit: print 'best V Lum = %0.2g' % (Vlumbest)
    print 'tstar = %0.1f; tau = %0.1f; tdust = %0.1f; thick = %0.2f; sluml = %0.1f; r1 = %0.1f; ebv = %0.3f; amin = %0.3f amax = %0.3f; Rv = %0.2f' % (tstarbest,taubest,tdbest,thickbest,slumlbest,r1best,ebvbest,aminbest,amaxbest,rvbest)
   
